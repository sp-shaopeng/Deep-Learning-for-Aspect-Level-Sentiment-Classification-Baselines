#Name of the file containing a feature template
#features=/home/hltcoe/mmitchell/ERMA/spanish/conll09/10-fold/all_args_all_roles/3/template.typed.ff
#Name of the file contating data
#data=/home/hltcoe/mmitchell/ERMA/spanish/conll09/10-fold/all_args_all_roles/3/train.typed.ff

#Name of the output file -- the paramters that minimized the training objective
#will be save in <out_ff>-best.ff
#out_ff=/home/hltcoe/mmitchell/ERMA/spanish/conll09/10-fold/all_args_all_roles/3/trained.typed
#output intermediate training results. 
#Those will be saved in <out_ff>-r<restart_num>-i<iter_num>.ff
out_iters=false
#Use micro average for training (testing)
use_micro_ave=false
#Use const-sensitive training
cost_sensitive=false
#Anneal for max-product BP
anneal=false
#Use sofmax-margin training
softmax=false
#Parameter for softmax-margin
C=0.5
#try restarts with different learning rates
rate_restart=false
#The tester should output scores for each training document (example)
output_doc_scores=false
#Loss function to be optimized (tested)
lfn=LogL
#Randomize the starting parameters (instead of using the weights in the input template)
rnd_fg=false
#The learn rate
learn_rate=.1
#The optimization algorithm (currenlty SGD (stochastic gradient descend)
#or SMD (stochastic meta descend))
opt_alg=SGD
#number of training iterations
iter=20
#maximum number of iterations for BP
maxiter=100
#Stopping epsilon for BP -- stop when max residual<tol
tol=1e-5
#Temperature for softmaxes
beta=2.0
#regularizer strength
reg_beta=0.0
#regularizer function (currently Zero, L1 or L2)
reg_func=Zero
#For annealing softmaxes -- start and end temperature and number of bucket values
start_temp=1.0
end_temp=1.0
num_buckets=1
#sizes of mini-batch for the optimization algorithm
batch_size=5
#number of random restarts (only works when rndm_fg=true)
num_restarts=1
#lambda and mu parameters for SMD
lambda=.5
mu=.6
#use max_prod for testing
max_prod=false
#verbosity level
verbose=0

